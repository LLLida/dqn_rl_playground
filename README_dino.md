# Учим динозаврика прыгать

Среда: динозаврик(агент) имеет два действия: ничего не делать и
прыгнуть. Навстречу ему летят препятствия, он должен избежать их.

Стейт: 4 числа. Сделал так для простоты, конечно, хотелось бы
скармливать картинки и не думать.

Текущий метод обучения: простая feed-forward на три слоя с temporal
difference. Loss - квадрат ошибки. Используется prioritized experience
replay и target network.

## Хотелось бы попробовать

 - использовать cross entropy вместо квадрата ошибки, есть подозрение,
   что это существенно улучшит процесс обучения;

 - стейт - картинка, а не маленький вектор

 - что-то модное, чего я не знаю

 - policy gradient (вряд ли)
